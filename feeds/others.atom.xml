<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Xcodest - Others</title><link href="http://xcodest.me/" rel="alternate"></link><link href="http://xcodest.me/feeds/others.atom.xml" rel="self"></link><id>http://xcodest.me/</id><updated>2017-02-23T00:00:00+08:00</updated><entry><title>Nova Cell V2 详解</title><link href="http://xcodest.me/nova-cell-v2.html" rel="alternate"></link><published>2017-02-23T00:00:00+08:00</published><updated>2017-02-23T00:00:00+08:00</updated><author><name>Jeffrey4l</name></author><id>tag:xcodest.me,2017-02-23:/nova-cell-v2.html</id><summary type="html">&lt;p&gt; 现在 ，OpenStack  在控制平面上的性能瓶颈主要在  Message Queue  和  Database 。 尤其是  Message Queue ,  随着计算节点的增加 ， 性能变的越来越差 。 为了应对这种情况 ， Nova  很早之前提出来  nova-cell ( 以下以  cellv1  代替 )   的解决方案 。 目的是在把大的  OpenStack  集群分成小的单元 ， 每个单元有自己的  Message Queue  和  Database。 以此来解决规模增加时引起的性能问题 。 而且不会向  Region  那样 ， 把各个集群独立运行 。 在  cell  里面 ，Keystone、Neutron、Cinder、Glance  等资源还是共享的 。&lt;/p&gt;
&lt;div class="section" id="cell-v1"&gt;
&lt;h2&gt;cell v1&lt;/h2&gt;
&lt;p&gt;cellv1  最初的想法很好 ， 但是局限于早期  nova  的架构 ， 硬生生的加个  nova-cell …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt; 现在 ，OpenStack  在控制平面上的性能瓶颈主要在  Message Queue  和  Database 。 尤其是  Message Queue ,  随着计算节点的增加 ， 性能变的越来越差 。 为了应对这种情况 ， Nova  很早之前提出来  nova-cell ( 以下以  cellv1  代替 )   的解决方案 。 目的是在把大的  OpenStack  集群分成小的单元 ， 每个单元有自己的  Message Queue  和  Database。 以此来解决规模增加时引起的性能问题 。 而且不会向  Region  那样 ， 把各个集群独立运行 。 在  cell  里面 ，Keystone、Neutron、Cinder、Glance  等资源还是共享的 。&lt;/p&gt;
&lt;div class="section" id="cell-v1"&gt;
&lt;h2&gt;cell v1&lt;/h2&gt;
&lt;p&gt;cellv1  最初的想法很好 ， 但是局限于早期  nova  的架构 ， 硬生生的加个  nova-cell  服务来在各个  cell  间传递消息 ， 使得架构更加复杂 。 以下是  cellv1  的架构 &lt;/p&gt;
&lt;img alt="nova cell v1 architecture" class="align-center" src="images/nova-cell/nova-cell-v1-arch.jpg" style="width: 371px; height: 374px;" /&gt;
&lt;p&gt;cell v1  的问题在于 ：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt; 一直以来 ，cell v1  被标记为实验性质 &lt;/li&gt;
&lt;li&gt; 相关的测试很少 ， 而且也没有  v1 + neutron  的测试 &lt;/li&gt;
&lt;li&gt; 现在来说功能已经冻结 ， 不会加入新的功能 &lt;/li&gt;
&lt;li&gt; 不严重的  Bug  根本不会去修复 &lt;/li&gt;
&lt;li&gt; 使用案例很少 。 现在经常提到的使用案例也只有 CERN（ 欧洲原子能研究中心 ）。 一般规模下 ， 完全没有必要搭建  cell v1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; 所以 ， 现在进行部署的话 ， 如果用  cell,  就尽量使用  cell v2  吧 。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cell-v2"&gt;
&lt;h2&gt;cell v2&lt;/h2&gt;
&lt;p&gt;cell v2  自  Newton  版本引入 ，Ocata  版本变为必要组件 。 以后默认部署都会初始化一个单  cell  的架构 。&lt;/p&gt;
&lt;p&gt;cell v2  的架构图如下 ， 看着比  cell v1  清爽不少 。&lt;/p&gt;
&lt;img alt="nova cell v2 architecture" class="align-center" src="images/nova-cell/nova-cell-v2-arch.jpg" style="width: 554px; height: 319px;" /&gt;
&lt;p&gt; 从架构图上 ， 可以看到 ：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;api  和  cell  有了明显的边界 。 api  层面只需要数据库 ， 不需要  Message Queue。&lt;ul&gt;
&lt;li&gt;nova-api  现在依赖  nova_api  和  nova_cell0  两个数据库 。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nova-scheduler  服务只需要在  api  层面上安装 ，cell  不需要参数调度 。 这样实现了一次调度就可以确定到具体在哪个  cell  的哪台机器上启动 &lt;ul&gt;
&lt;li&gt; 这里其实依赖  placement  服务 ,  以后的文章会提到 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cell  里面只需要安装  nova-compute  和  nova-conductor  服务 ， 和其依赖的  DB  和  MQ&lt;/li&gt;
&lt;li&gt; 所有的  cell  变成一个扁平架构 。 比之前的多层父子架构要简化很多 。&lt;/li&gt;
&lt;li&gt;api 上面服务会直接连接  cell  的  MQ  和  DB,  所以不需要类似  nova-cell  这样子的额外服务存在 。 性能上也会有及大的提升 &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="nova-api-nova-cell0"&gt;
&lt;h3&gt;nova_api &amp;amp; nova_cell0&lt;/h3&gt;
&lt;p&gt; 自  Newton  版本 ，nova  就一直拆分  nova  数据库 ，  为  cell v2  做准备 。  把一些全局数据表从  nova  库搬到了  nova_api,  下面是现在  nova_api  里面的所有表 。&lt;/p&gt;
&lt;pre class="literal-block"&gt;
+------------------------------+     +------------------------------+
| Tables_in_nova_api           |     | Tables_in_nova_api           |
+------------------------------+     +------------------------------+
| aggregate_hosts              |     | inventories                  |
| aggregate_metadata           |     | key_pairs                    |
| aggregates                   |     | migrate_version              |
| allocations                  |     | placement_aggregates         |
| build_requests               |     | project_user_quotas          |
| cell_mappings                |     | quota_classes                |
| flavor_extra_specs           |     | quota_usages                 |
| flavor_projects              |     | quotas                       |
| flavors                      |     | request_specs                |
| host_mappings                |     | reservations                 |
| instance_group_member        |     | resource_classes             |
| instance_group_policy        |     | resource_provider_aggregates |
| instance_groups              |     | resource_providers           |
| instance_mappings            |     |                              |
+------------------------------+     +------------------------------+
&lt;/pre&gt;
&lt;p&gt; 可以看到像  flavor, instance groups, quota  这些表已经迁移了过来 。&lt;/p&gt;
&lt;p&gt;nova_cell0  数据库的  schema  和  nova  是一样的 ， 他存在的只要用途是 ： 当  instance  调度失败时 ， instance  的信息不属于任何一个  cell,  所以放到  cell0  上面 。 因此里面的数据并不是太重要 。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cell-related-tables"&gt;
&lt;h3&gt;Cell Related Tables&lt;/h3&gt;
&lt;p&gt;Cell  相关的数据库表都在  nova_api  里面 ， 包括  cell_mappings, host_mappings, instance_mappings。 其表结构如下 ：&lt;/p&gt;
&lt;img alt="" class="align-center" src="images/nova-cell/nova-cell-v2-uml.jpg" style="width: 391px; height: 204px;" /&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;cell_mappings  表  cell  的  Database  和  Mesage Queue  的连接 。 用于和子  cell  通讯 &lt;/li&gt;
&lt;li&gt;host_mappings  是用于  nova-scheduler,  可以确认分配到的机器 。 这里其实也有一个坑 ， 之前  nova-compute  启动起来 ， 就可以直接使用了 ，cell v2  之后 ， 就需要手动运行  &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;nova-manage&lt;/span&gt; cell_v2 discover_host&lt;/tt&gt; ,  把  host mapping  到  cell_mappings  表里面 ， 那台计算节点才会加入到调度中 。&lt;/li&gt;
&lt;li&gt;instance_mappings  表里有所有  instance id,  这样在查询  instance  时 ， 就可以从这个表里查到他所在的  cell,  然后直连  cell  拿到  instance  具体信息 。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="cell"&gt;
&lt;h3&gt;cell  流程 &lt;/h3&gt;
&lt;pre class="literal-block"&gt;
                        api/cell boundary
                                +
 nova show &amp;lt;uuid&amp;gt;               |
             |                  |
             v      3           |
        nova-api+--------------------&amp;gt;cell-db
         +     +                |
         |     +----+           |
        1|          | 2         |      1. Determine which cell the instance is in
         v          v           |      2. Get db connection for cell
instance_mapping  cell_mapping  |      3. Query cell db for data
                                +
&lt;/pre&gt;
&lt;p&gt; 当想要获取一个机器的详细信息时 :&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;nova-api  先从  instance_mappings  表拿到  instance  的  cell_id&lt;/li&gt;
&lt;li&gt; 再从  cell_mappings  表拿到所在  cell  的  DB connection&lt;/li&gt;
&lt;li&gt; 直接连接  cell   的  DB  拿到机器的详细信息 &lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="literal-block"&gt;
                        api/cell boundary
                                +
 nova reboot &amp;lt;uuid&amp;gt;             |
              +                 |
              |                 |
              v    3            |
        nova-api+--------------------&amp;gt;cell-mq+-----&amp;gt;compute
         +     +                |
         |     +---+            |
        1|         | 2          |      1. Determine which cell the instance is in
         v         v            |      2. Get mq connection for cell
instance_mapping cell_mapping   |      3. Send RPC message to compute
                                +
&lt;/pre&gt;
&lt;p&gt; 当要重启一台机器时 ：&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;nova-api  先从  instance_mappings  表里拿到  instance  所在的  cell_id&lt;/li&gt;
&lt;li&gt; 从  cell_mappings  里拿到所在  cell  的  message queue  连接 &lt;/li&gt;
&lt;li&gt;nova-api  直接给  mq  的相关队列发重启机器的消息 &lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="literal-block"&gt;
                        api/cell boundary
                                +
 nova boot  ...                 |
         +        3             |
         |    +----------------------&amp;gt;cell-db
         v    +   4             |
        nova-api+--------------------&amp;gt;cell-mq+-&amp;gt;conductor+-&amp;gt;compute
         +    +                 |
         |    +-------------+   |
        2|     1            |   |
         v                  |   |       1. Schedule the instance
instance_mapping            |   |       2. Record which cell the instance was scheduled to
                            |   +       3. Create instance record
                            v           4. Send RPC message to conductor to build
                       scheduling
&lt;/pre&gt;
&lt;p&gt; 当新建机器时 :&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;nova-api  接到用户的请求信息 ， 先转发到  nova-scheduler  进行调度 ，
nova-scheduler  通过  placement service,  直接确定分配到哪台机器上 &lt;/li&gt;
&lt;li&gt;nova-api  把  instance  的信息存入 instance_mappings  表 &lt;/li&gt;
&lt;li&gt;nova-api  把机器信息存到目标  cell  的  database&lt;/li&gt;
&lt;li&gt;nova-api  给  cell  的  message queue  的相关队列发消息 ， 启动机器 &lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h3&gt;Cell v2  的优点 &lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt; 数据库和消息队列作为  nova  的一等公民 。&lt;/li&gt;
&lt;li&gt; 在  cell  的数据库里没有冗余数据 ， 所有共享数据都在  nova-api  中 &lt;/li&gt;
&lt;li&gt; 全局数据和  cell  数据有一条清晰的界线 &lt;/li&gt;
&lt;li&gt; 非  cell  用户很容易的就可以迁移到  cell v2  上面 。 不需要更改现在的部署架构 &lt;/li&gt;
&lt;li&gt;cell v1  的用户也可以迁移到  cell v2  上 。 只要手动建立起所有的 mapping,  关掉现在存在的  nova-cell  服务 ， 清掉最上层  cell  的数据库 。 但是最上层  cell  本质上和其它  cell  是不同的 。  所以需要调整架构 &lt;/li&gt;
&lt;li&gt; 增减  cell  变的十分简单 ， 而且在把某个 cell  加入之前 ， 可以在其它环境进行测试 &lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h3&gt;Cell v2  相关命令 &lt;/h3&gt;
&lt;p&gt; 因为  cell v2  完全靠  database  的操作为建立 ， 所以也没有相关的  api  接口 。  主要靠  nova-manage cell_v2  命令 。 详细说明参见  &lt;a class="footnote-reference" href="#id7" id="id3"&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;pre class="literal-block"&gt;
nova-manage cell_v2

    create_cell
    delete_cell
    list_cells

    map_cell0
    discover_hosts
    simple_cell_setup

    map_cell_and_hosts
    map_instances
    verify_instance
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="id4"&gt;
&lt;h3&gt; 其它 &lt;/h3&gt;
&lt;div class="section" id="id5"&gt;
&lt;h4&gt; 计算节点自动发现 &lt;/h4&gt;
&lt;p&gt; 上面提到了现在  nova-compute  服务上线后 ， 不会自动加到  nova-api  的  host_mappings  里面 ， 也就不会加到  nova-scheduler  的调度中 。  需要手动运行  &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;nova-manage&lt;/span&gt; cell_v2 discover_hosts&lt;/tt&gt;  命令 。 这显示略显繁琐 。&lt;/p&gt;
&lt;p&gt; 在小型一些的环境上 ， 推荐打开自动发现功能 ， 就不用手动跑命令了 。&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[scheduler]
discover_hosts_in_cells_interval=-1

#This value controls how often (in seconds) the scheduler should attempt
#to discover new hosts that have been added to cells. If negative (the
#default), no automatic discovery will occur.
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="id6"&gt;
&lt;h4&gt; 性能分析 &lt;/h4&gt;
&lt;p&gt; 为了拿到  instance  的详细信息 ， 需要查询  nova_api  数据库 ， 相比之前要多查询一次数据库 (  虽然是有三个表 ， 但是可以用多表连接查询 ， 一次就可以拿到所有的结果  )。 但是一来数据相当少 ， 而且很容易加上一层  cache,  并不会对性造成什么影响 。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="kolla"&gt;
&lt;h3&gt;Kolla  实现 &lt;/h3&gt;
&lt;p&gt; 现在  Kolla  已经支持自动部署一个基本的  cell  环境 ， 而且支持从没有  cell  的  Newton  升级到有  cell  的  Ocata  版本 。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="ref"&gt;
&lt;h2&gt;REF&lt;/h2&gt;
&lt;table class="docutils footnote" frame="void" id="id7" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://docs.openstack.org/developer/nova/man/nova-manage.html#nova-cells-v2"&gt;https://docs.openstack.org/developer/nova/man/nova-manage.html#nova-cells-v2&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id8" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[2]&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://www.openstack.org/videos/video/nova-cells-v2-whats-going-on"&gt;Presentation that Andrew Laski gave at the Austin (Newton) summit&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id9" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[3]&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://docs.openstack.org/developer/nova/cells.html"&gt;https://docs.openstack.org/developer/nova/cells.html&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id10" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[4]&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://paste.openstack.org/show/144068/"&gt;Flow chart&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id11" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[5]&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://wiki.openstack.org/wiki/Nova-Cells-v2"&gt;https://wiki.openstack.org/wiki/Nova-Cells-v2&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id12" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[6]&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://www.ustack.com/news/what-is-nova-cells-v2/"&gt;https://www.ustack.com/news/what-is-nova-cells-v2/&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id13" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[7]&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://www.cnblogs.com/wanglm/articles/5749813.html"&gt;http://www.cnblogs.com/wanglm/articles/5749813.html&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="OpenStack"></category><category term="Kolla"></category></entry><entry><title>Ansible with-items auto flatten list of lists</title><link href="http://xcodest.me/ansible-with-items-auto-flatten-list-of-lists.html" rel="alternate"></link><published>2016-12-27T00:00:00+08:00</published><updated>2016-12-27T00:00:00+08:00</updated><author><name>Jeffrey4l</name></author><id>tag:xcodest.me,2016-12-27:/ansible-with-items-auto-flatten-list-of-lists.html</id><summary type="html">&lt;p&gt;Ansible has &lt;tt class="docutils literal"&gt;with_items&lt;/tt&gt; directive, which make task iterating the lists in
&lt;tt class="docutils literal"&gt;with_items&lt;/tt&gt;. A normal usage is like&lt;/p&gt;
&lt;pre class="literal-block"&gt;
- name: copy file
  copy: src={{ item }} dest=/root/.ssh/{{ item }}
  with_items:
    - id_rsa
    - id_rsa.pub
&lt;/pre&gt;
&lt;p&gt;But if you use list of lists, it will not work as your expect.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
- name: copy file
  copy: src …&lt;/pre&gt;</summary><content type="html">&lt;p&gt;Ansible has &lt;tt class="docutils literal"&gt;with_items&lt;/tt&gt; directive, which make task iterating the lists in
&lt;tt class="docutils literal"&gt;with_items&lt;/tt&gt;. A normal usage is like&lt;/p&gt;
&lt;pre class="literal-block"&gt;
- name: copy file
  copy: src={{ item }} dest=/root/.ssh/{{ item }}
  with_items:
    - id_rsa
    - id_rsa.pub
&lt;/pre&gt;
&lt;p&gt;But if you use list of lists, it will not work as your expect.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
- name: copy file
  copy: src={{ item.0 }} dest=/root/.ssh/{{ item.1 }}
  with_items:
    - [ id_rsa, id_rsa_foo ]
    - [ id_rsa.pub, rd_rsa_foo.pub ]
&lt;/pre&gt;
&lt;p&gt;Ansible will flatten the lists just like what &lt;tt class="docutils literal"&gt;with_flatten&lt;/tt&gt; does. As a
result, it will work like&lt;/p&gt;
&lt;pre class="literal-block"&gt;
- name: copy file
  copy: src={{ item.0 }} dest=/root/.ssh/{{ item.1 }}
  with_items:
    - id_rsa
    - id_rsa_foo
    - id_rsa.pub
    - id_rsa_foo.pub
&lt;/pre&gt;
&lt;p&gt;This is not what we want, of curse. The workaround is wrapping the list of
lists. like:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
- name: copy file
  copy: src={{ item.0 }} dest=/root/.ssh/{{ item.1 }}
  with_items:
    -
      - [ id_rsa, id_rsa_foo ]
      - [ id_rsa.pub, rd_rsa_foo.pub ]
&lt;/pre&gt;
&lt;p&gt;This is a historical issue rather than a bug. Here is the related ansible
bug[0] and doc fix[1].&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;[0] &lt;a class="reference external" href="https://github.com/ansible/ansible/issues/5913"&gt;https://github.com/ansible/ansible/issues/5913&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[1] &lt;a class="reference external" href="https://github.com/ko-zu/ansible/commit/0a1a5cde86df0424441b78dd9e67b96159cfd70f"&gt;https://github.com/ko-zu/ansible/commit/0a1a5cde86df0424441b78dd9e67b96159cfd70f&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content><category term="Ansible"></category></entry><entry><title>CentOS 根分区自动扩展</title><link href="http://xcodest.me/centos-root-partition-auto-grow.html" rel="alternate"></link><published>2016-11-14T00:00:00+08:00</published><updated>2016-11-14T00:00:00+08:00</updated><author><name>Jeffrey4l</name></author><id>tag:xcodest.me,2016-11-14:/centos-root-partition-auto-grow.html</id><summary type="html">&lt;p&gt;CentOS 5  太老了 ， 完全不支持 。&lt;/p&gt;
&lt;p&gt;CentOS 6  要实现分区自动扩展 ， 要安装以下三个包 &lt;/p&gt;
&lt;pre class="literal-block"&gt;
yum install cloud-init cloud-utils-growpart dracut-modules-growroot
#  生新生成  initramfs
dracut -f
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dracut&lt;/tt&gt;  把  &lt;tt class="docutils literal"&gt;growroot&lt;/tt&gt;  的脚本封装到  &lt;tt class="docutils literal"&gt;initramfs&lt;/tt&gt;  里面 。  把系统启动时 ，
&lt;tt class="docutils literal"&gt;initramfs&lt;/tt&gt;  利用  &lt;tt class="docutils literal"&gt;growpart&lt;/tt&gt;  命令把根分区进行扩展 。 然后启动真正的 
&lt;tt class="docutils literal"&gt;kernel&lt;/tt&gt;,  之后  &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;cloud-init&lt;/span&gt;&lt;/tt&gt;  服务会自动把文件系统进行扩展 。&lt;/p&gt;
&lt;p&gt; 之所以使用  &lt;tt class="docutils literal"&gt;initramfs&lt;/tt&gt;  这种方式 ， 原因 [0]：&lt;/p&gt;
&lt;blockquote&gt;
Growpart called by cloud-init only works for kernels &amp;gt;3.8. Only newer …&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;CentOS 5  太老了 ， 完全不支持 。&lt;/p&gt;
&lt;p&gt;CentOS 6  要实现分区自动扩展 ， 要安装以下三个包 &lt;/p&gt;
&lt;pre class="literal-block"&gt;
yum install cloud-init cloud-utils-growpart dracut-modules-growroot
#  生新生成  initramfs
dracut -f
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dracut&lt;/tt&gt;  把  &lt;tt class="docutils literal"&gt;growroot&lt;/tt&gt;  的脚本封装到  &lt;tt class="docutils literal"&gt;initramfs&lt;/tt&gt;  里面 。  把系统启动时 ，
&lt;tt class="docutils literal"&gt;initramfs&lt;/tt&gt;  利用  &lt;tt class="docutils literal"&gt;growpart&lt;/tt&gt;  命令把根分区进行扩展 。 然后启动真正的 
&lt;tt class="docutils literal"&gt;kernel&lt;/tt&gt;,  之后  &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;cloud-init&lt;/span&gt;&lt;/tt&gt;  服务会自动把文件系统进行扩展 。&lt;/p&gt;
&lt;p&gt; 之所以使用  &lt;tt class="docutils literal"&gt;initramfs&lt;/tt&gt;  这种方式 ， 原因 [0]：&lt;/p&gt;
&lt;blockquote&gt;
Growpart called by cloud-init only works for kernels &amp;gt;3.8. Only newer
kernels support changing the partition size of a mounted partition. When
using an older kernel the resizing of the root partition happens in the
initrd stage before the root partition is mounted and the subsequent
cloud-init growpart run is a no-op.&lt;/blockquote&gt;
&lt;p&gt;CentOS 7  使用的是  3.10 ( &amp;gt; 3.8 )  的内核 ， 所以并不需要  dracut-modules-growroot
 包 (  源里面也并没有这个包  )。  只安装以下两个包就可以了 。&lt;/p&gt;
&lt;pre class="literal-block"&gt;
yum install cloud-init cloud-utils-growpart
&lt;/pre&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;[0] &lt;a class="reference external" href="http://openstack.openstack.narkive.com/opyLuPqC/centos-6-5-cloud-init-growpart-resizefs-does-not-work-on-first-boot"&gt;http://openstack.openstack.narkive.com/opyLuPqC/centos-6-5-cloud-init-growpart-resizefs-does-not-work-on-first-boot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content><category term="Linux"></category><category term="OpenStack"></category></entry><entry><title>Docker init 进程</title><link href="http://xcodest.me/docker-init-process.html" rel="alternate"></link><published>2016-11-07T00:00:00+08:00</published><updated>2016-11-07T00:00:00+08:00</updated><author><name>Jeffrey4l</name></author><id>tag:xcodest.me,2016-11-07:/docker-init-process.html</id><summary type="html">&lt;p&gt; 应用容器化后 ， 重启容器的时候 ， 经常会很慢 ， 而且 docker daemon  日志中经常会抛出以 
 下错误 &lt;/p&gt;
&lt;pre class="literal-block"&gt;
dockerd[559]: msg=&amp;quot;Container 5054f failed to exit within 10 seconds of
signal 15 - using the force&amp;quot;
&lt;/pre&gt;
&lt;p&gt; 默认的的  signal 15  根本就没有使其退出 ， 最后还是  10  秒超时后强制退出 (kill) 的 。 而 
 且有时还会出现大量僵尸进程 &lt;/p&gt;
&lt;p&gt; 这可不是一个好现象 。 本文解释其原因及解决方法 。&lt;/p&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt; 背景知识 &lt;/h2&gt;
&lt;div class="section" id="id2"&gt;
&lt;h3&gt; 信号 &lt;/h3&gt;
&lt;p&gt; 这个是  Linux  最常见一个概念 ， 一般杀死进程时都会用到  &lt;tt class="docutils literal"&gt;kill &amp;lt;pid&amp;gt;&lt;/tt&gt; 。  不同的信 
 号有不同的默认行为 。 用户可以注册自己的信号处理函数 ， 来覆盖掉默认行为 …&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt; 应用容器化后 ， 重启容器的时候 ， 经常会很慢 ， 而且 docker daemon  日志中经常会抛出以 
 下错误 &lt;/p&gt;
&lt;pre class="literal-block"&gt;
dockerd[559]: msg=&amp;quot;Container 5054f failed to exit within 10 seconds of
signal 15 - using the force&amp;quot;
&lt;/pre&gt;
&lt;p&gt; 默认的的  signal 15  根本就没有使其退出 ， 最后还是  10  秒超时后强制退出 (kill) 的 。 而 
 且有时还会出现大量僵尸进程 &lt;/p&gt;
&lt;p&gt; 这可不是一个好现象 。 本文解释其原因及解决方法 。&lt;/p&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt; 背景知识 &lt;/h2&gt;
&lt;div class="section" id="id2"&gt;
&lt;h3&gt; 信号 &lt;/h3&gt;
&lt;p&gt; 这个是  Linux  最常见一个概念 ， 一般杀死进程时都会用到  &lt;tt class="docutils literal"&gt;kill &amp;lt;pid&amp;gt;&lt;/tt&gt; 。  不同的信 
 号有不同的默认行为 。 用户可以注册自己的信号处理函数 ， 来覆盖掉默认行为 。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id3"&gt;
&lt;h3&gt; 僵尸进程 &lt;/h3&gt;
&lt;p&gt; 僵尸进程是终止运行的进程 ， 为什么它们是有害的 ?&lt;/p&gt;
&lt;p&gt; 虽然应用申请的内存已经释放了 ， 但是你依然能通过  &lt;tt class="docutils literal"&gt;ps&lt;/tt&gt;  看到它 。 这是因为有一些内 
 核资源没有释放 。 下面是  Linux &lt;tt class="docutils literal"&gt;waitpid&lt;/tt&gt;  的  man page:&lt;/p&gt;
&lt;blockquote&gt;
As long as a zombie is not removed from the system via a wait, it will
consume a slot in the kernel process table, and if this table fills, it
will not be possible to create further processes.&amp;quot;&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id4"&gt;
&lt;h2&gt; 容器化后的问题 &lt;/h2&gt;
&lt;p&gt; 容器化后 ， 由于单容器单进程 ， 已经没有传统意义上的  init  进程了 。 应用进程直接占用 
 了  pid 1  的进程号 。 从而导致以下两个问题 。&lt;/p&gt;
&lt;div class="section" id="id5"&gt;
&lt;h3&gt; 进程不能正常终止 &lt;/h3&gt;
&lt;p&gt;Linux  内核中会对  pid 1  进程发送特殊的信号量 。&lt;/p&gt;
&lt;p&gt; 一般情况下 ， 当给一个进程发送信号时 ， 内核会先检查是否有用户定义的处理函数 ， 如果 
 没有 ， 就会回退到默认行为 。 例如使用  SIGTERM  直接杀死进程 。 然而 ， 如果进程的  PID
 是  1,  内核会特殊对待它 。 如果没有没有注册用户处理函数 ， 内核不会回退到默认行为 ，
 什么也不做 。 换句话说 ， 如果你的进程没有处理信号的函数 ， 给他发送  &lt;tt class="docutils literal"&gt;SIGTERM&lt;/tt&gt;  会一 
 点效果也没有 。&lt;/p&gt;
&lt;p&gt; 常见的使用是  docker run my-container script.  给  &lt;tt class="docutils literal"&gt;docker run&lt;/tt&gt;  进程发送 
&lt;tt class="docutils literal"&gt;SIGTERM&lt;/tt&gt;  信号会杀掉  &lt;tt class="docutils literal"&gt;docker run&lt;/tt&gt;  进程 ， 但是容器还在后台运行 。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id6"&gt;
&lt;h3&gt; 孤儿僵尸进程不能正常回收 &lt;/h3&gt;
&lt;p&gt; 当进程退出时 ， 它会变成僵尸进程 ， 直到它的父进程调用  &lt;tt class="docutils literal"&gt;wait()&lt;/tt&gt; (  或其变种  )  的系 
 统调用 。process table  里面会把它的标记为  &lt;tt class="docutils literal"&gt;defunct&lt;/tt&gt;  状态 。 一般情况下 ， 父进程应 
 该立即调用  &lt;tt class="docutils literal"&gt;wait()&lt;/tt&gt;,  以防僵尸进程时间过长 。&lt;/p&gt;
&lt;p&gt; 如果父进程在子进程之前退出 ， 子进程会变成孤儿进程 ,  它的父进程会变成  PID 1。 因此 
，init  进程就要对这些进程负责 ， 并在适当的时候调用  &lt;tt class="docutils literal"&gt;wait()&lt;/tt&gt;  方法 。&lt;/p&gt;
&lt;p&gt; 但是 ， 通常情况下 ， 大部分进程不会处理偶然依附在自己进程上的随机子进程 ， 所以在容器 
 中 ， 会出现许多僵尸进程 。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id7"&gt;
&lt;h2&gt; 解决方案 &lt;/h2&gt;
&lt;p&gt; 让所有的应用能正确的处理以上的情况 ， 不太现实 。 好在现在有很多解决方案 ， 例如 
dumb-init [0] 。 他像是一个小型  init  服务 ， 他启动一个子进程并转发所有接收到的信 
 号量给子进程 。 而且不需要修改应用代码 。&lt;/p&gt;
&lt;p&gt; 此时你的应用进程已经不是  pid 1  了 ， 所以已经没有上面提到的问题 。 而且  dumb-init
 也会转发所有的信号给子进程 ， 应用的形为和在没有  dumb-init  时是一样的 。 如果应用进 
 程死掉了 ，dumb-init  进程也会死掉 ， 并会清理所有其它的子进程 。&lt;/p&gt;
&lt;p&gt; 使用方法如下 ,  在  Dockerfile  里面加上 ：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# install dumb-init
RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64
RUN chmod +x /usr/local/bin/dumb-init

# Runs &amp;quot;/usr/bin/dumb-init -- /my/script --with --args&amp;quot;
ENTRYPOINT [&amp;quot;/usr/bin/dumb-init&amp;quot;, &amp;quot;--&amp;quot;]
CMD [&amp;quot;/my/script&amp;quot;, &amp;quot;--with&amp;quot;, &amp;quot;--args&amp;quot;]
&lt;/pre&gt;
&lt;p&gt; 类似的方案  tini [1], pidunu[3]&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="kolla"&gt;
&lt;h2&gt;Kolla  相关 &lt;/h2&gt;
&lt;p&gt;Kolla  最近已经发布了  newton  版本的  release 。  已经加上了  dumb-init  的解决方案 。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id8"&gt;
&lt;h2&gt; 参考资料 &lt;/h2&gt;
&lt;p&gt;[0] &lt;a class="reference external" href="https://github.com/Yelp/dumb-init"&gt;https://github.com/Yelp/dumb-init&lt;/a&gt;
[1] &lt;a class="reference external" href="https://github.com/krallin/tini"&gt;https://github.com/krallin/tini&lt;/a&gt;
[2] &lt;a class="reference external" href="https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem"&gt;https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem&lt;/a&gt;
[3] &lt;a class="reference external" href="https://github.com/rciorba/pidunu"&gt;https://github.com/rciorba/pidunu&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</content><category term="Linux"></category><category term="Docker"></category></entry><entry><title>Reset Password in Systemd</title><link href="http://xcodest.me/reset-password-in-systemd.html" rel="alternate"></link><published>2016-08-28T10:00:00+08:00</published><updated>2016-08-28T10:00:00+08:00</updated><author><name>Jeffrey4l</name></author><id>tag:xcodest.me,2016-08-28:/reset-password-in-systemd.html</id><summary type="html">&lt;p&gt; 现在基本是用  ssh key  来登录系统了 。 之前可以直接在  GRUB  直接进单用户改密码 。 使用了  systemd  的系统 ， 后已经不可以这么使用了 。 原因是  Systemd  的单用户模式使用了  &lt;tt class="docutils literal"&gt;/usr/sbin/sulogin&lt;/tt&gt;  这个  shell,  也必须输入密码才可以 。&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# /usr/lib/systemd/system/rescue.service
[Unit]
Description=Rescue Shell
Documentation=man:sulogin(8)
DefaultDependencies=no
Conflicts=shutdown.target
After=sysinit.target plymouth-start.service
Before=shutdown.target

[Service]
Environment=HOME …&lt;/pre&gt;</summary><content type="html">&lt;p&gt; 现在基本是用  ssh key  来登录系统了 。 之前可以直接在  GRUB  直接进单用户改密码 。 使用了  systemd  的系统 ， 后已经不可以这么使用了 。 原因是  Systemd  的单用户模式使用了  &lt;tt class="docutils literal"&gt;/usr/sbin/sulogin&lt;/tt&gt;  这个  shell,  也必须输入密码才可以 。&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# /usr/lib/systemd/system/rescue.service
[Unit]
Description=Rescue Shell
Documentation=man:sulogin(8)
DefaultDependencies=no
Conflicts=shutdown.target
After=sysinit.target plymouth-start.service
Before=shutdown.target

[Service]
Environment=HOME=/root
WorkingDirectory=-/root
ExecStartPre=-/bin/plymouth --wait quit
ExecStartPre=-/bin/echo -e 'You are in rescue mode. After logging in, type &amp;quot;journalctl -xb&amp;quot; to view\\nsystem logs, &amp;quot;systemctl reboot&amp;quot; to reboot, &amp;quot;systemctl default&amp;quot; or ^D to\\nboot into default mode.'
ExecStart=-/bin/sh -c &amp;quot;/usr/bin/sulogin; /usr/bin/systemctl --job-mode=fail --no-block default&amp;quot;
Type=idle
StandardInput=tty-force
StandardOutput=inherit
StandardError=inherit
KillMode=process
IgnoreSIGPIPE=no
SendSIGHUP=yes
&lt;/pre&gt;
&lt;p&gt; 自  systemd 215  版本后 ， 新加了一个  &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;systemd.debug-shell&lt;/span&gt;&lt;/tt&gt;  的内核参数 ， 内容如下 ：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# /usr/lib/systemd/system/debug-shell.service
[Unit]
Description=Early root shell on /dev/tty9 FOR DEBUGGING ONLY
Documentation=man:sushell(8)
DefaultDependencies=no
IgnoreOnIsolate=yes
ConditionPathExists=/dev/tty9

[Service]
Environment=TERM=linux
ExecStart=/bin/sh
Restart=always
RestartSec=0
StandardInput=tty
TTYPath=/dev/tty9
TTYReset=yes
TTYVHangup=yes
KillMode=process
IgnoreSIGPIPE=no
# bash ignores SIGTERM
KillSignal=SIGHUP

# Unset locale for the console getty since the console has problems
# displaying some internationalized messages.
Environment=LANG= LANGUAGE= LC_CTYPE= LC_NUMERIC= LC_TIME= LC_COLLATE= LC_MONETARY= LC_MESSAGES= LC_PAPER= LC_NAME= LC_ADDRESS= LC_TELEPHONE= LC_MEASUREMENT= LC_IDENTIFICATION=

[Install]
WantedBy=sysinit.target
&lt;/pre&gt;
&lt;p&gt; 可以看到 ，systemd  直接在  &lt;tt class="docutils literal"&gt;tty9&lt;/tt&gt;  上面开了一个  sh,  不用密码就可以登录 。&lt;/p&gt;
&lt;p&gt; 使用方法很简单 ， 在  GRUB  界面上 ， 输入  &lt;tt class="docutils literal"&gt;e&lt;/tt&gt;  进入编辑模式 ， 找到  &lt;tt class="docutils literal"&gt;linux16&lt;/tt&gt;  那一行 ， 在后面加入  &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;systemd.debug-shell&lt;/span&gt;&lt;/tt&gt;  就可以了 。 之后  &lt;tt class="docutils literal"&gt;Ctrl + x&lt;/tt&gt;  继续启动就可以了 。 启动完成后 ，&lt;tt class="docutils literal"&gt;Ctrl + Alt + F9&lt;/tt&gt;  进入  &lt;tt class="docutils literal"&gt;tty9&lt;/tt&gt;，  就可以欢快的改密码了 &lt;/p&gt;
</content><category term="Linux"></category><category term="Systemd"></category></entry></feed>